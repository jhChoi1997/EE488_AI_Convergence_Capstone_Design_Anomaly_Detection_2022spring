{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EE488_DCASE2020_WaveNet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMzsUgwwjo35+X/odO8Kd6N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhChoi1997/EE488_AI_Convergence_Capstone_Design_Anomaly_Detection_2022spring/blob/main/EE488_DCASE2020_WaveNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pk3FQfxnb5eZ"
      },
      "outputs": [],
      "source": [
        "!gdown https://drive.google.com/uc?id=1p0aANQlQRKqM9FGhkV3j2h55PJUOgEXg\n",
        "!unzip valve.zip -d ./valve/\n",
        "\n",
        "!gdown https://drive.google.com/uc?id=1hKmdy5bySo5JrZ9FjxVrrBp_5XhgWe2Y\n",
        "!unzip valve.zip -d ./valve_test/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import librosa\n",
        "import librosa.core\n",
        "import librosa.feature\n",
        "import yaml\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "UyOOR7c-b-rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = './valve'\n",
        "test_dir = './valve_test'\n",
        "model_dir = './model'\n",
        "\n",
        "n_fft = 2048\n",
        "hop_length = 512\n",
        "n_mels = 128\n",
        "power = 2\n",
        "n_block = 5\n",
        "n_mul = 6\n",
        "kernel_size = 3\n",
        "\n",
        "EPOCHS = 10\n",
        "BATCH = 32"
      ],
      "metadata": {
        "id": "6_4utXv77bLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def file_load(wav_name):\n",
        "  try:\n",
        "    return librosa.load(wav_name, sr=None, mono=False)\n",
        "  except:\n",
        "    print('file_broken or not exists!! : {}'.format(wav_name))\n",
        "    \n",
        "\n",
        "def file_list_generator(target_dir):\n",
        "  training_list_path = os.path.abspath('{dir}/*.wav'.format(dir=target_dir))\n",
        "  files = sorted(glob.glob(training_list_path))\n",
        "  if len(files) == 0:\n",
        "    print('no_wav_file!!')\n",
        "  return files\n",
        "\n",
        "\n",
        "def file_to_log_mel(file_name, n_mels, n_fft, hop_length, power):\n",
        "  y, sr = file_load(file_name)\n",
        "  mel_spectrogram = librosa.feature.melspectrogram(y=y,\n",
        "                                                   sr=sr,\n",
        "                                                   n_fft=n_fft,\n",
        "                                                   hop_length=hop_length,\n",
        "                                                   n_mels=n_mels,\n",
        "                                                   power=power)\n",
        "  \n",
        "  log_mel_spectrogram = 20.0 / power * np.log10(mel_spectrogram + sys.float_info.epsilon)\n",
        "\n",
        "  return log_mel_spectrogram\n",
        "\n",
        "\n",
        "def list_to_dataset(file_list, n_mels, n_fft, hop_length, power):\n",
        "  for idx in tqdm(range(len(file_list))):\n",
        "    log_mel = file_to_log_mel(file_list[idx],\n",
        "                              n_mels=n_mels,\n",
        "                              n_fft=n_fft,\n",
        "                              hop_length=hop_length,\n",
        "                              power=power)\n",
        "    if idx == 0:\n",
        "      dataset = np.zeros((len(file_list), len(log_mel[:,0]), len(log_mel[0,:])), float)\n",
        "    dataset[idx, :, :] = log_mel\n",
        "  \n",
        "  return dataset"
      ],
      "metadata": {
        "id": "bky8dvUJ9sJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "dataset_dir = os.path.abspath(dataset_dir)\n",
        "machine_type = os.path.split(dataset_dir)[1]\n",
        "model_file_path = '{model}/model_{machine_type}'.format(model=model_dir, machine_type=machine_type)\n",
        "\n",
        "files = file_list_generator(dataset_dir)\n",
        "train_data = list_to_dataset(files,\n",
        "                             n_mels=n_mels,\n",
        "                             n_fft=n_fft,\n",
        "                             hop_length=hop_length,\n",
        "                             power=power)"
      ],
      "metadata": {
        "id": "rd_XVNnxBeQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torch.Tensor(train_data)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using {device} device')"
      ],
      "metadata": {
        "id": "a-BwQnsLDeEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset.shape)"
      ],
      "metadata": {
        "id": "LCUmsMONstd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalConv1d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1):\n",
        "        super(CausalConv1d, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dilation = dilation\n",
        "\n",
        "        self.conv1 = self.causal_conv(self.in_channels, self.out_channels, self.kernel_size, self.dilation)\n",
        "        self.padding = self.conv1.padding[0]\n",
        "\n",
        "    def causal_conv(self, in_channels, out_channels, kernel_size, dilation):\n",
        "        pad = (kernel_size - 1) * dilation\n",
        "        return nn.Conv1d(in_channels, out_channels, kernel_size, padding=pad, dilation=dilation)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = x[:, :, :-self.padding]\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, n_channel, n_mul, kernel_size, dilation_rate):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.n_channel = n_channel\n",
        "        self.n_mul = n_mul\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dilation_rate = dilation_rate\n",
        "        self.n_filter = self.n_channel * self.n_mul\n",
        "\n",
        "        self.sigmoid_group_norm = nn.GroupNorm(1, self.n_filter)\n",
        "        self.sigmoid_conv = CausalConv1d(self.n_filter, self.n_filter, self.kernel_size, self.dilation_rate)\n",
        "        self.tanh_group_norm = nn.GroupNorm(1, self.n_filter)\n",
        "        self.tanh_conv = CausalConv1d(self.n_filter, self.n_filter, self.kernel_size, self.dilation_rate)\n",
        "\n",
        "        self.skip_group_norm = nn.GroupNorm(1, self.n_filter).to(device)\n",
        "        self.skip_conv = nn.Conv1d(self.n_filter, self.n_channel, 1)\n",
        "        self.residual_group_norm = nn.GroupNorm(1, self.n_filter)\n",
        "        self.residual_conv = nn.Conv1d(self.n_filter, self.n_filter, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.sigmoid_group_norm(x)\n",
        "        x1 = self.sigmoid_conv(x1)\n",
        "        x2 = self.tanh_group_norm(x)\n",
        "        x2 = self.tanh_conv(x2)\n",
        "        x1 = nn.Sigmoid()(x1)\n",
        "        x2 = nn.Tanh()(x2)\n",
        "        x = x1 * x2\n",
        "\n",
        "        x1 = self.skip_group_norm(x)\n",
        "        skip = self.skip_conv(x1)\n",
        "        x2 = self.residual_group_norm(x)\n",
        "        residual = self.residual_conv(x2)\n",
        "\n",
        "        return skip, residual\n",
        "\n",
        "\n",
        "class WaveNet(nn.Module):\n",
        "    def __init__(self, n_block, n_channel, n_mul, kernel_size):\n",
        "        super(WaveNet, self).__init__()\n",
        "\n",
        "        self.n_block = n_block\n",
        "        self.n_channel = n_channel\n",
        "        self.n_mul = n_mul\n",
        "        self.kernel_size = kernel_size\n",
        "        self.n_filter = self.n_channel * self.n_mul\n",
        "\n",
        "        self.group_norm1 = nn.GroupNorm(1, self.n_channel)\n",
        "        self.conv1 = nn.Conv1d(self.n_channel, self.n_filter, 1)\n",
        "\n",
        "        self.residual_blocks = [ResidualBlock(self.n_channel, self.n_mul, self.kernel_size, 2 ** i) for i in\n",
        "                                range(self.n_block)]\n",
        "\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        self.group_norm2 = nn.GroupNorm(1, self.n_channel)\n",
        "        self.conv2 = nn.Conv1d(self.n_channel, self.n_channel, 1)\n",
        "        self.group_norm3 = nn.GroupNorm(1, self.n_channel)\n",
        "        self.conv3 = nn.Conv1d(self.n_channel, self.n_channel, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.group_norm1(x)\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        skip_connections = []\n",
        "        for rb in self.residual_blocks:\n",
        "            rb = rb.to(device)\n",
        "            skip, x = rb(x)\n",
        "            skip_connections.append(skip)\n",
        "        skip_x = torch.stack(skip_connections).sum(dim=0)\n",
        "\n",
        "        x = self.relu1(skip_x)\n",
        "        x = self.group_norm2(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.group_norm3(x)\n",
        "        x = self.conv3(x)\n",
        "        output = x[:, :, self.get_receptive_field() - 1:-1]\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_receptive_field(self):\n",
        "        receptive_field = 1\n",
        "        for _ in range(self.n_block):\n",
        "            receptive_field = receptive_field * 2 + self.kernel_size - 2\n",
        "        return receptive_field\n"
      ],
      "metadata": {
        "id": "gGMxCpboD1Ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = WaveNet(n_block, n_mels, n_mul, kernel_size).to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "5Wc8gj8fD3Xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "4XbU_Muwylp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, X in enumerate(dataloader):\n",
        "    X = X.to(device)\n",
        "\n",
        "    pred = model(X)\n",
        "    receptive_field = model.get_receptive_field()\n",
        "\n",
        "    loss = loss_fn(pred, X[:, :, receptive_field:])\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 30 == 0:\n",
        "      loss, current = loss.item(), batch * len(X)\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ],
      "metadata": {
        "id": "HfAgtA5VysRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(EPOCHS):\n",
        "  print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
        "  train(train_dataloader, model, loss_fn, optimizer)"
      ],
      "metadata": {
        "id": "wBLfnI2faIEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_anomaly_score(true, pred):\n",
        "  anomaly_score = nn.MSELoss()(true, pred)\n",
        "  return anomaly_score\n"
      ],
      "metadata": {
        "id": "P-P6IJ9eAaRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_files = sorted(glob.glob('{dir}/normal_*'.format(dir=test_dir)))\n",
        "anomaly_files = sorted(glob.glob('{dir}/anomaly_*'.format(dir=test_dir)))\n",
        "\n",
        "normal_labels = np.zeros(len(normal_files))\n",
        "anomaly_labels = np.ones(len(anomaly_files))\n",
        "\n",
        "test_files = np.concatenate((normal_files, anomaly_files), axis=0)\n",
        "y_true = np.concatenate((normal_labels, anonmaly_labels), axis=0)\n",
        "y_pred = [0. for k in test_files]\n",
        "\n",
        "test_dataset = list_to_dataset(test_files, n_mels, n_fft, hop_length, power)\n",
        "\n",
        "for file_idx in range(len(test_files)):\n",
        "  data = test_dataset[file_idx].to(device)\n",
        "  output = model(data)\n",
        "\n",
        "  score = get_anomaly_score(data, output)\n",
        "  y_pred[file_idx] = score\n",
        "\n",
        "auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "\n",
        "print(auc)"
      ],
      "metadata": {
        "id": "PefFvOVLAXfZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}