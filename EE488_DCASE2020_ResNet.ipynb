{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EE488_DCASE2020_ResNet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMvbP7npXe+OXBKbMJvXj4S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhChoi1997/EE488_AI_Convergence_Capstone_Design_Anomaly_Detection_2022spring/blob/main/EE488_DCASE2020_ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/MyDrive/EE488/'\n",
        "os.chdir(root_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmsyEVpT-TCm",
        "outputId": "1b0ec156-425c-4e3d-e82b-35877d6f3224"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import librosa\n",
        "import librosa.core\n",
        "import librosa.feature\n",
        "import yaml\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "AoN8IfDDe7eo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = './valve'\n",
        "test_dir = './valve_test'\n",
        "model_dir = './model'\n",
        "\n",
        "n_fft = 2048\n",
        "hop_length = 512\n",
        "n_mels = 128\n",
        "power = 2\n",
        "n_mul = 6\n",
        "kernel_size = 3\n",
        "\n",
        "EPOCHS = 10\n",
        "BATCH = 32"
      ],
      "metadata": {
        "id": "irWngkVQe9xh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def file_load(wav_name):\n",
        "  try:\n",
        "    return librosa.load(wav_name, sr=None, mono=False)\n",
        "  except:\n",
        "    print('file_broken or not exists!! : {}'.format(wav_name))\n",
        "    \n",
        "\n",
        "def file_list_generator(target_dir):\n",
        "  training_list_path = os.path.abspath('{dir}/*.wav'.format(dir=target_dir))\n",
        "  files = sorted(glob.glob(training_list_path))\n",
        "  if len(files) == 0:\n",
        "    print('no_wav_file!!')\n",
        "  return files\n",
        "\n",
        "\n",
        "def file_to_log_mel(file_name, n_mels, n_fft, hop_length, power):\n",
        "  y, sr = file_load(file_name)\n",
        "  mel_spectrogram = librosa.feature.melspectrogram(y=y,\n",
        "                                                   sr=sr,\n",
        "                                                   n_fft=n_fft,\n",
        "                                                   hop_length=hop_length,\n",
        "                                                   n_mels=n_mels,\n",
        "                                                   power=power)\n",
        "  \n",
        "  log_mel_spectrogram = 20.0 / power * np.log10(mel_spectrogram + sys.float_info.epsilon)\n",
        "\n",
        "  return log_mel_spectrogram\n",
        "\n",
        "\n",
        "def list_to_dataset(file_list, n_mels, n_fft, hop_length, power):\n",
        "  for idx in tqdm(range(len(file_list))):\n",
        "    log_mel = file_to_log_mel(file_list[idx],\n",
        "                              n_mels=n_mels,\n",
        "                              n_fft=n_fft,\n",
        "                              hop_length=hop_length,\n",
        "                              power=power)\n",
        "    if idx == 0:\n",
        "      dataset = np.zeros((len(file_list), 1, len(log_mel[:,0]), len(log_mel[0,:])), float)\n",
        "    dataset[idx, 0, :, :] = log_mel\n",
        "  \n",
        "  return dataset"
      ],
      "metadata": {
        "id": "pVzxLpwJfCHD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "dataset_dir = os.path.abspath(dataset_dir)\n",
        "machine_type = os.path.split(dataset_dir)[1]\n",
        "model_file_path = '{model}/model_{machine_type}'.format(model=model_dir, machine_type=machine_type)\n",
        "\n",
        "files = file_list_generator(dataset_dir)\n",
        "train_data = list_to_dataset(files,\n",
        "                             n_mels=n_mels,\n",
        "                             n_fft=n_fft,\n",
        "                             hop_length=hop_length,\n",
        "                             power=power)\n",
        "train_data = torch.Tensor(train_data)\n",
        "\n",
        "label_list = ['id_00', 'id_02', 'id_04', 'id_06']\n",
        "\n",
        "train_label = torch.LongTensor([idx for file_name in files for idx, label_idx in enumerate(label_list) if label_idx in file_name])\n",
        "train_label = nn.functional.one_hot(train_label, num_classes=len(label_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZ4hUQgnfGR8",
        "outputId": "83c81bbd-a26c-437e-f747-bd7e9dcd7e32"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3291/3291 [02:31<00:00, 21.79it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtCxI-pUxRW-",
        "outputId": "56b9968f-3f3b-4291-fd3f-240d16ad5d17"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 0, 0, 0],\n",
              "        [1, 0, 0, 0],\n",
              "        [1, 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, 1],\n",
              "        [0, 0, 0, 1],\n",
              "        [0, 0, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(train_data, train_label)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH, shuffle=True)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using {device} device')"
      ],
      "metadata": {
        "id": "AFSekkQOH8k1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e1d259a-67bb-409c-8cb6-2fab38b611f2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, projection=False):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.in_channel = in_channel\n",
        "        self.out_channel = out_channel\n",
        "        self.projection = projection\n",
        "        if self.projection:\n",
        "            self.conv1 = nn.Conv2d(self.in_channel, self.out_channel, kernel_size=3, stride=2, padding=(1, 1))\n",
        "        else:\n",
        "            self.conv1 = nn.Conv2d(self.in_channel, self.out_channel, kernel_size=3, padding=(1, 1))\n",
        "        self.bn1 = nn.BatchNorm2d(self.out_channel)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(self.out_channel, self.out_channel, kernel_size=3, padding='same')\n",
        "        self.bn2 = nn.BatchNorm2d(self.out_channel)\n",
        "        if self.projection:\n",
        "            self.downsample = nn.Conv2d(self.in_channel, self.out_channel, stride=2, kernel_size=1)\n",
        "        else:\n",
        "            self.downsample = nn.Conv2d(self.in_channel, self.out_channel, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.projection:\n",
        "            skip = self.downsample(x)\n",
        "        else:\n",
        "            skip = x\n",
        "\n",
        "        out += skip\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, n_class):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.n_channel = 8\n",
        "        self.n_class = n_class\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, self.n_channel, kernel_size=7, stride=2, padding=(3, 2))\n",
        "        self.bn1 = nn.BatchNorm2d(self.n_channel)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pooling1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=(1, 1))\n",
        "\n",
        "        self.block1 = ResidualBlock(self.n_channel, self.n_channel)\n",
        "        self.block2 = ResidualBlock(self.n_channel, self.n_channel)\n",
        "        self.block3 = ResidualBlock(self.n_channel, self.n_channel * 2, True)\n",
        "        self.block4 = ResidualBlock(self.n_channel * 2, self.n_channel * 2)\n",
        "        self.block5 = ResidualBlock(self.n_channel * 2, self.n_channel * 4, True)\n",
        "        self.block6 = ResidualBlock(self.n_channel * 4, self.n_channel * 4)\n",
        "        self.block7 = ResidualBlock(self.n_channel * 4, self.n_channel * 8, True)\n",
        "        self.block8 = ResidualBlock(self.n_channel * 8, self.n_channel * 8)\n",
        "\n",
        "        self.gap1 = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(self.n_channel * 8, self.n_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pooling1(x)\n",
        "\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        x = self.block5(x)\n",
        "        x = self.block6(x)\n",
        "        x = self.block7(x)\n",
        "        x = self.block8(x)\n",
        "\n",
        "        x = self.gap1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "omvJNh1cJGr6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet(len(label_list)).to(device)"
      ],
      "metadata": {
        "id": "sKZ8h5oZTS6H"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "eB4P4abcTluk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, mdoel, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X = X.to(device)\n",
        "    y = y.float().to(device)\n",
        "\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 30 == 0:\n",
        "      loss, current = loss.item(), batch * len(X)\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ],
      "metadata": {
        "id": "ssxgb3WET-xo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(EPOCHS):\n",
        "  print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
        "  train(train_dataloader, model, loss_fn, optimizer)"
      ],
      "metadata": {
        "id": "bim4mnLYUd75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6d44568-0f53-4da5-9090-9e1a627dcdbe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.507398  [    0/ 3291]\n",
            "loss: 0.360370  [  960/ 3291]\n",
            "loss: 0.059299  [ 1920/ 3291]\n",
            "loss: 0.024497  [ 2880/ 3291]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.053262  [    0/ 3291]\n",
            "loss: 0.015846  [  960/ 3291]\n",
            "loss: 0.003955  [ 1920/ 3291]\n",
            "loss: 0.003126  [ 2880/ 3291]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.005282  [    0/ 3291]\n",
            "loss: 0.001708  [  960/ 3291]\n",
            "loss: 0.002747  [ 1920/ 3291]\n",
            "loss: 0.001000  [ 2880/ 3291]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.000549  [    0/ 3291]\n",
            "loss: 0.001906  [  960/ 3291]\n",
            "loss: 0.001021  [ 1920/ 3291]\n",
            "loss: 0.000538  [ 2880/ 3291]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.000322  [    0/ 3291]\n",
            "loss: 0.000263  [  960/ 3291]\n",
            "loss: 0.001160  [ 1920/ 3291]\n",
            "loss: 0.000373  [ 2880/ 3291]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.000622  [    0/ 3291]\n",
            "loss: 0.003613  [  960/ 3291]\n",
            "loss: 0.000312  [ 1920/ 3291]\n",
            "loss: 0.000580  [ 2880/ 3291]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.000229  [    0/ 3291]\n",
            "loss: 0.001111  [  960/ 3291]\n",
            "loss: 0.027643  [ 1920/ 3291]\n",
            "loss: 0.002264  [ 2880/ 3291]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.001102  [    0/ 3291]\n",
            "loss: 0.003079  [  960/ 3291]\n",
            "loss: 0.006265  [ 1920/ 3291]\n",
            "loss: 0.000468  [ 2880/ 3291]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.001007  [    0/ 3291]\n",
            "loss: 0.000820  [  960/ 3291]\n",
            "loss: 0.001419  [ 1920/ 3291]\n",
            "loss: 0.000331  [ 2880/ 3291]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.003517  [    0/ 3291]\n",
            "loss: 0.000119  [  960/ 3291]\n",
            "loss: 0.000635  [ 1920/ 3291]\n",
            "loss: 0.000246  [ 2880/ 3291]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.000107  [    0/ 3291]\n",
            "loss: 0.000441  [  960/ 3291]\n",
            "loss: 0.000473  [ 1920/ 3291]\n",
            "loss: 0.000263  [ 2880/ 3291]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.000369  [    0/ 3291]\n",
            "loss: 0.000214  [  960/ 3291]\n",
            "loss: 0.000137  [ 1920/ 3291]\n",
            "loss: 0.001218  [ 2880/ 3291]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.000052  [    0/ 3291]\n",
            "loss: 0.000117  [  960/ 3291]\n",
            "loss: 0.000206  [ 1920/ 3291]\n",
            "loss: 0.000270  [ 2880/ 3291]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.000239  [    0/ 3291]\n",
            "loss: 0.001145  [  960/ 3291]\n",
            "loss: 0.000099  [ 1920/ 3291]\n",
            "loss: 0.000037  [ 2880/ 3291]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.000026  [    0/ 3291]\n",
            "loss: 0.000070  [  960/ 3291]\n",
            "loss: 0.000043  [ 1920/ 3291]\n",
            "loss: 0.000022  [ 2880/ 3291]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.000054  [    0/ 3291]\n",
            "loss: 0.000036  [  960/ 3291]\n",
            "loss: 0.000081  [ 1920/ 3291]\n",
            "loss: 0.000059  [ 2880/ 3291]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.000029  [    0/ 3291]\n",
            "loss: 0.000518  [  960/ 3291]\n",
            "loss: 0.000085  [ 1920/ 3291]\n",
            "loss: 0.000085  [ 2880/ 3291]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.000042  [    0/ 3291]\n",
            "loss: 0.000069  [  960/ 3291]\n",
            "loss: 0.000111  [ 1920/ 3291]\n",
            "loss: 0.000077  [ 2880/ 3291]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.000285  [    0/ 3291]\n",
            "loss: 0.000561  [  960/ 3291]\n",
            "loss: 0.000037  [ 1920/ 3291]\n",
            "loss: 0.000026  [ 2880/ 3291]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.000048  [    0/ 3291]\n",
            "loss: 0.000032  [  960/ 3291]\n",
            "loss: 0.000011  [ 1920/ 3291]\n",
            "loss: 0.000035  [ 2880/ 3291]\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.000064  [    0/ 3291]\n",
            "loss: 0.000544  [  960/ 3291]\n",
            "loss: 0.000021  [ 1920/ 3291]\n",
            "loss: 0.000019  [ 2880/ 3291]\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.000024  [    0/ 3291]\n",
            "loss: 0.000026  [  960/ 3291]\n",
            "loss: 0.000014  [ 1920/ 3291]\n",
            "loss: 0.000057  [ 2880/ 3291]\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.000009  [    0/ 3291]\n",
            "loss: 0.000016  [  960/ 3291]\n",
            "loss: 0.000122  [ 1920/ 3291]\n",
            "loss: 0.000038  [ 2880/ 3291]\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.000053  [    0/ 3291]\n",
            "loss: 0.000011  [  960/ 3291]\n",
            "loss: 0.000012  [ 1920/ 3291]\n",
            "loss: 0.000028  [ 2880/ 3291]\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.000054  [    0/ 3291]\n",
            "loss: 0.000008  [  960/ 3291]\n",
            "loss: 0.000007  [ 1920/ 3291]\n",
            "loss: 0.000006  [ 2880/ 3291]\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.000047  [    0/ 3291]\n",
            "loss: 0.000005  [  960/ 3291]\n",
            "loss: 0.000011  [ 1920/ 3291]\n",
            "loss: 0.000006  [ 2880/ 3291]\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.000025  [    0/ 3291]\n",
            "loss: 0.000014  [  960/ 3291]\n",
            "loss: 0.000010  [ 1920/ 3291]\n",
            "loss: 0.000030  [ 2880/ 3291]\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.000211  [    0/ 3291]\n",
            "loss: 0.000005  [  960/ 3291]\n",
            "loss: 0.000007  [ 1920/ 3291]\n",
            "loss: 0.000007  [ 2880/ 3291]\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.000044  [    0/ 3291]\n",
            "loss: 0.000009  [  960/ 3291]\n",
            "loss: 0.000008  [ 1920/ 3291]\n",
            "loss: 0.000006  [ 2880/ 3291]\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.000014  [    0/ 3291]\n",
            "loss: 0.000022  [  960/ 3291]\n",
            "loss: 0.000017  [ 1920/ 3291]\n",
            "loss: 0.000025  [ 2880/ 3291]\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.000157  [    0/ 3291]\n",
            "loss: 0.000006  [  960/ 3291]\n",
            "loss: 0.000009  [ 1920/ 3291]\n",
            "loss: 0.000015  [ 2880/ 3291]\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.000047  [    0/ 3291]\n",
            "loss: 0.000007  [  960/ 3291]\n",
            "loss: 0.000023  [ 1920/ 3291]\n",
            "loss: 0.000002  [ 2880/ 3291]\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.000046  [    0/ 3291]\n",
            "loss: 0.000009  [  960/ 3291]\n",
            "loss: 0.000006  [ 1920/ 3291]\n",
            "loss: 0.000005  [ 2880/ 3291]\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.000024  [    0/ 3291]\n",
            "loss: 0.000004  [  960/ 3291]\n",
            "loss: 0.000013  [ 1920/ 3291]\n",
            "loss: 0.000003  [ 2880/ 3291]\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.000001  [    0/ 3291]\n",
            "loss: 0.000005  [  960/ 3291]\n",
            "loss: 0.000007  [ 1920/ 3291]\n",
            "loss: 0.000013  [ 2880/ 3291]\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.000010  [    0/ 3291]\n",
            "loss: 0.000270  [  960/ 3291]\n",
            "loss: 0.000004  [ 1920/ 3291]\n",
            "loss: 0.000002  [ 2880/ 3291]\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.000006  [    0/ 3291]\n",
            "loss: 0.000007  [  960/ 3291]\n",
            "loss: 0.000002  [ 1920/ 3291]\n",
            "loss: 0.000004  [ 2880/ 3291]\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.000006  [    0/ 3291]\n",
            "loss: 0.000006  [  960/ 3291]\n",
            "loss: 0.000002  [ 1920/ 3291]\n",
            "loss: 0.000011  [ 2880/ 3291]\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.000006  [    0/ 3291]\n",
            "loss: 0.000021  [  960/ 3291]\n",
            "loss: 0.000002  [ 1920/ 3291]\n",
            "loss: 0.000007  [ 2880/ 3291]\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.000008  [    0/ 3291]\n",
            "loss: 0.000025  [  960/ 3291]\n",
            "loss: 0.000003  [ 1920/ 3291]\n",
            "loss: 0.000009  [ 2880/ 3291]\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.000004  [    0/ 3291]\n",
            "loss: 0.000003  [  960/ 3291]\n",
            "loss: 0.000003  [ 1920/ 3291]\n",
            "loss: 0.000002  [ 2880/ 3291]\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.000002  [    0/ 3291]\n",
            "loss: 0.000020  [  960/ 3291]\n",
            "loss: 0.000003  [ 1920/ 3291]\n",
            "loss: 0.000007  [ 2880/ 3291]\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.000002  [    0/ 3291]\n",
            "loss: 0.000002  [  960/ 3291]\n",
            "loss: 0.000001  [ 1920/ 3291]\n",
            "loss: 0.000004  [ 2880/ 3291]\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.000003  [    0/ 3291]\n",
            "loss: 0.000004  [  960/ 3291]\n",
            "loss: 0.000009  [ 1920/ 3291]\n",
            "loss: 0.000002  [ 2880/ 3291]\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.000003  [    0/ 3291]\n",
            "loss: 0.000014  [  960/ 3291]\n",
            "loss: 0.000002  [ 1920/ 3291]\n",
            "loss: 0.000001  [ 2880/ 3291]\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.000010  [    0/ 3291]\n",
            "loss: 0.000004  [  960/ 3291]\n",
            "loss: 0.000001  [ 1920/ 3291]\n",
            "loss: 0.000742  [ 2880/ 3291]\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.000005  [    0/ 3291]\n",
            "loss: 0.000002  [  960/ 3291]\n",
            "loss: 0.000002  [ 1920/ 3291]\n",
            "loss: 0.000002  [ 2880/ 3291]\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.000001  [    0/ 3291]\n",
            "loss: 0.000015  [  960/ 3291]\n",
            "loss: 0.000001  [ 1920/ 3291]\n",
            "loss: 0.000001  [ 2880/ 3291]\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.000005  [    0/ 3291]\n",
            "loss: 0.000004  [  960/ 3291]\n",
            "loss: 0.000003  [ 1920/ 3291]\n",
            "loss: 0.000004  [ 2880/ 3291]\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.000001  [    0/ 3291]\n",
            "loss: 0.000003  [  960/ 3291]\n",
            "loss: 0.000003  [ 1920/ 3291]\n",
            "loss: 0.000012  [ 2880/ 3291]\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.000006  [    0/ 3291]\n",
            "loss: 0.000002  [  960/ 3291]\n",
            "loss: 0.000007  [ 1920/ 3291]\n",
            "loss: 0.000003  [ 2880/ 3291]\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.000021  [    0/ 3291]\n",
            "loss: 0.000000  [  960/ 3291]\n",
            "loss: 0.000012  [ 1920/ 3291]\n",
            "loss: 0.000020  [ 2880/ 3291]\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 0.000004  [    0/ 3291]\n",
            "loss: 0.000000  [  960/ 3291]\n",
            "loss: 0.000002  [ 1920/ 3291]\n",
            "loss: 0.000001  [ 2880/ 3291]\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 0.000002  [    0/ 3291]\n",
            "loss: 0.000005  [  960/ 3291]\n",
            "loss: 0.000002  [ 1920/ 3291]\n",
            "loss: 0.000001  [ 2880/ 3291]\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.000001  [    0/ 3291]\n",
            "loss: 0.000003  [  960/ 3291]\n",
            "loss: 0.006573  [ 1920/ 3291]\n",
            "loss: 0.009225  [ 2880/ 3291]\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.002907  [    0/ 3291]\n",
            "loss: 0.000330  [  960/ 3291]\n",
            "loss: 0.000648  [ 1920/ 3291]\n",
            "loss: 0.000821  [ 2880/ 3291]\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 0.001379  [    0/ 3291]\n",
            "loss: 0.000495  [  960/ 3291]\n",
            "loss: 0.000307  [ 1920/ 3291]\n",
            "loss: 0.000991  [ 2880/ 3291]\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 0.003791  [    0/ 3291]\n",
            "loss: 0.000935  [  960/ 3291]\n",
            "loss: 0.002147  [ 1920/ 3291]\n",
            "loss: 0.000114  [ 2880/ 3291]\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 0.004817  [    0/ 3291]\n",
            "loss: 0.000188  [  960/ 3291]\n",
            "loss: 0.000071  [ 1920/ 3291]\n",
            "loss: 0.000093  [ 2880/ 3291]\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 0.000271  [    0/ 3291]\n",
            "loss: 0.000065  [  960/ 3291]\n",
            "loss: 0.000046  [ 1920/ 3291]\n",
            "loss: 0.000075  [ 2880/ 3291]\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "loss: 0.000410  [    0/ 3291]\n",
            "loss: 0.000088  [  960/ 3291]\n",
            "loss: 0.000083  [ 1920/ 3291]\n",
            "loss: 0.000065  [ 2880/ 3291]\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "loss: 0.000141  [    0/ 3291]\n",
            "loss: 0.000279  [  960/ 3291]\n",
            "loss: 0.000027  [ 1920/ 3291]\n",
            "loss: 0.001041  [ 2880/ 3291]\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "loss: 0.000043  [    0/ 3291]\n",
            "loss: 0.000034  [  960/ 3291]\n",
            "loss: 0.001041  [ 1920/ 3291]\n",
            "loss: 0.000396  [ 2880/ 3291]\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "loss: 0.000009  [    0/ 3291]\n",
            "loss: 0.000011  [  960/ 3291]\n",
            "loss: 0.000020  [ 1920/ 3291]\n",
            "loss: 0.000060  [ 2880/ 3291]\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "loss: 0.000012  [    0/ 3291]\n",
            "loss: 0.000022  [  960/ 3291]\n",
            "loss: 0.000082  [ 1920/ 3291]\n",
            "loss: 0.000018  [ 2880/ 3291]\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "loss: 0.000149  [    0/ 3291]\n",
            "loss: 0.000037  [  960/ 3291]\n",
            "loss: 0.000046  [ 1920/ 3291]\n",
            "loss: 0.000007  [ 2880/ 3291]\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "loss: 0.000025  [    0/ 3291]\n",
            "loss: 0.000033  [  960/ 3291]\n",
            "loss: 0.000017  [ 1920/ 3291]\n",
            "loss: 0.000018  [ 2880/ 3291]\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "loss: 0.000006  [    0/ 3291]\n",
            "loss: 0.000216  [  960/ 3291]\n",
            "loss: 0.000018  [ 1920/ 3291]\n",
            "loss: 0.000017  [ 2880/ 3291]\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "loss: 0.000007  [    0/ 3291]\n",
            "loss: 0.000029  [  960/ 3291]\n",
            "loss: 0.000016  [ 1920/ 3291]\n",
            "loss: 0.000036  [ 2880/ 3291]\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "loss: 0.000044  [    0/ 3291]\n",
            "loss: 0.000019  [  960/ 3291]\n",
            "loss: 0.000006  [ 1920/ 3291]\n",
            "loss: 0.000004  [ 2880/ 3291]\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "loss: 0.000026  [    0/ 3291]\n",
            "loss: 0.000026  [  960/ 3291]\n",
            "loss: 0.000001  [ 1920/ 3291]\n",
            "loss: 0.000016  [ 2880/ 3291]\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "loss: 0.000133  [    0/ 3291]\n",
            "loss: 0.000028  [  960/ 3291]\n",
            "loss: 0.000039  [ 1920/ 3291]\n",
            "loss: 0.000008  [ 2880/ 3291]\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "loss: 0.000005  [    0/ 3291]\n",
            "loss: 0.000010  [  960/ 3291]\n",
            "loss: 0.000050  [ 1920/ 3291]\n",
            "loss: 0.000013  [ 2880/ 3291]\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "loss: 0.000007  [    0/ 3291]\n",
            "loss: 0.000003  [  960/ 3291]\n",
            "loss: 0.000005  [ 1920/ 3291]\n",
            "loss: 0.000097  [ 2880/ 3291]\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "loss: 0.000023  [    0/ 3291]\n",
            "loss: 0.000074  [  960/ 3291]\n",
            "loss: 0.000050  [ 1920/ 3291]\n",
            "loss: 0.000018  [ 2880/ 3291]\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "loss: 0.000002  [    0/ 3291]\n",
            "loss: 0.000010  [  960/ 3291]\n",
            "loss: 0.000009  [ 1920/ 3291]\n",
            "loss: 0.000024  [ 2880/ 3291]\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "loss: 0.000004  [    0/ 3291]\n",
            "loss: 0.000004  [  960/ 3291]\n",
            "loss: 0.000030  [ 1920/ 3291]\n",
            "loss: 0.000006  [ 2880/ 3291]\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "loss: 0.000024  [    0/ 3291]\n",
            "loss: 0.000049  [  960/ 3291]\n",
            "loss: 0.000003  [ 1920/ 3291]\n",
            "loss: 0.000037  [ 2880/ 3291]\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "loss: 0.000011  [    0/ 3291]\n",
            "loss: 0.000033  [  960/ 3291]\n",
            "loss: 0.000014  [ 1920/ 3291]\n",
            "loss: 0.000003  [ 2880/ 3291]\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "loss: 0.000003  [    0/ 3291]\n",
            "loss: 0.000007  [  960/ 3291]\n",
            "loss: 0.000017  [ 1920/ 3291]\n",
            "loss: 0.000010  [ 2880/ 3291]\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "loss: 0.000009  [    0/ 3291]\n",
            "loss: 0.000001  [  960/ 3291]\n",
            "loss: 0.000002  [ 1920/ 3291]\n",
            "loss: 0.000003  [ 2880/ 3291]\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "loss: 0.000041  [    0/ 3291]\n",
            "loss: 0.000015  [  960/ 3291]\n",
            "loss: 0.000008  [ 1920/ 3291]\n",
            "loss: 0.000014  [ 2880/ 3291]\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "loss: 0.000003  [    0/ 3291]\n",
            "loss: 0.000010  [  960/ 3291]\n",
            "loss: 0.000008  [ 1920/ 3291]\n",
            "loss: 0.000001  [ 2880/ 3291]\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "loss: 0.000006  [    0/ 3291]\n",
            "loss: 0.000068  [  960/ 3291]\n",
            "loss: 0.000003  [ 1920/ 3291]\n",
            "loss: 0.000002  [ 2880/ 3291]\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "loss: 0.000003  [    0/ 3291]\n",
            "loss: 0.000011  [  960/ 3291]\n",
            "loss: 0.000017  [ 1920/ 3291]\n",
            "loss: 0.000009  [ 2880/ 3291]\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "loss: 0.000010  [    0/ 3291]\n",
            "loss: 0.000016  [  960/ 3291]\n",
            "loss: 0.000003  [ 1920/ 3291]\n",
            "loss: 0.000001  [ 2880/ 3291]\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "loss: 0.000007  [    0/ 3291]\n",
            "loss: 0.000002  [  960/ 3291]\n",
            "loss: 0.000001  [ 1920/ 3291]\n",
            "loss: 0.000003  [ 2880/ 3291]\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "loss: 0.000002  [    0/ 3291]\n",
            "loss: 0.000001  [  960/ 3291]\n",
            "loss: 0.000003  [ 1920/ 3291]\n",
            "loss: 0.000011  [ 2880/ 3291]\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "loss: 0.000015  [    0/ 3291]\n",
            "loss: 0.000002  [  960/ 3291]\n",
            "loss: 0.000008  [ 1920/ 3291]\n",
            "loss: 0.000001  [ 2880/ 3291]\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "loss: 0.000004  [    0/ 3291]\n",
            "loss: 0.000003  [  960/ 3291]\n",
            "loss: 0.000003  [ 1920/ 3291]\n",
            "loss: 0.000200  [ 2880/ 3291]\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "loss: 0.000002  [    0/ 3291]\n",
            "loss: 0.000008  [  960/ 3291]\n",
            "loss: 0.000007  [ 1920/ 3291]\n",
            "loss: 0.000004  [ 2880/ 3291]\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "loss: 0.000004  [    0/ 3291]\n",
            "loss: 0.000002  [  960/ 3291]\n",
            "loss: 0.000010  [ 1920/ 3291]\n",
            "loss: 0.000014  [ 2880/ 3291]\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "loss: 0.000004  [    0/ 3291]\n",
            "loss: 0.000024  [  960/ 3291]\n",
            "loss: 0.000003  [ 1920/ 3291]\n",
            "loss: 0.000004  [ 2880/ 3291]\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "loss: 0.000004  [    0/ 3291]\n",
            "loss: 0.000001  [  960/ 3291]\n",
            "loss: 0.000051  [ 1920/ 3291]\n",
            "loss: 0.000000  [ 2880/ 3291]\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "loss: 0.000001  [    0/ 3291]\n",
            "loss: 0.000001  [  960/ 3291]\n",
            "loss: 0.000001  [ 1920/ 3291]\n",
            "loss: 0.000005  [ 2880/ 3291]\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "loss: 0.000001  [    0/ 3291]\n",
            "loss: 0.000001  [  960/ 3291]\n",
            "loss: 0.000001  [ 1920/ 3291]\n",
            "loss: 0.000001  [ 2880/ 3291]\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "loss: 0.000000  [    0/ 3291]\n",
            "loss: 0.000017  [  960/ 3291]\n",
            "loss: 0.000280  [ 1920/ 3291]\n",
            "loss: 0.000007  [ 2880/ 3291]\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "loss: 0.000002  [    0/ 3291]\n",
            "loss: 0.000032  [  960/ 3291]\n",
            "loss: 0.000003  [ 1920/ 3291]\n",
            "loss: 0.000022  [ 2880/ 3291]\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "loss: 0.000004  [    0/ 3291]\n",
            "loss: 0.000001  [  960/ 3291]\n",
            "loss: 0.000002  [ 1920/ 3291]\n",
            "loss: 0.000006  [ 2880/ 3291]\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "loss: 0.000000  [    0/ 3291]\n",
            "loss: 0.000002  [  960/ 3291]\n",
            "loss: 0.000003  [ 1920/ 3291]\n",
            "loss: 0.000002  [ 2880/ 3291]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_anomaly_score(true, pred):\n",
        "  anomaly_score = nn.CrossEntropyLoss()(pred, true)\n",
        "  return anomaly_score"
      ],
      "metadata": {
        "id": "ShvcaNJ3zoJm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_files = sorted(glob.glob('{dir}/normal_*'.format(dir=test_dir)))\n",
        "anomaly_files = sorted(glob.glob('{dir}/anomaly_*'.format(dir=test_dir)))\n",
        "\n",
        "normal_labels = np.zeros(len(normal_files))\n",
        "anomaly_labels = np.ones(len(anomaly_files))\n",
        "\n",
        "test_files = np.concatenate((normal_files, anomaly_files), axis=0)\n",
        "y_true = np.concatenate((normal_labels, anomaly_labels), axis=0)\n",
        "y_pred = [0. for k in test_files]"
      ],
      "metadata": {
        "id": "9ohMRZTfz-3i"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = list_to_dataset(test_files, n_mels, n_fft, hop_length, power)\n",
        "\n",
        "test_label = torch.LongTensor([idx for file_name in test_files for idx, label_idx in enumerate(label_list) if label_idx in file_name])\n",
        "test_label = nn.functional.one_hot(test_label, num_classes=len(label_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgBK3APD0BY5",
        "outputId": "09227a57-9429-404f-bfe1-c6fb3c05c925"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 160/160 [00:05<00:00, 27.57it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGSGnr3m1wcw",
        "outputId": "9c01168e-208b-42b0-8fcf-b5aca864c10d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(160, 1, 128, 313)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for file_idx in tqdm(range(len(test_files)), desc='test data '):\n",
        "  data = torch.Tensor(test_dataset[file_idx]).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "  output = model(data)\n",
        "  true = test_label[file_idx].float().unsqueeze(0).to(device)\n",
        "  score = get_anomaly_score(true, output)\n",
        "\n",
        "  y_pred[file_idx] = score.cpu().detach().numpy()\n",
        "\n",
        "auc = metrics.roc_auc_score(y_true, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWfFtdRB0xXK",
        "outputId": "743407f1-2693-44cf-ff55-8910c0a6320a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test data : 100%|██████████| 160/160 [00:02<00:00, 72.02it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7M1EMZJh2FMw",
        "outputId": "bf2b8383-168b-4620-a384-45f69efc8359"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.70765625"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    }
  ]
}